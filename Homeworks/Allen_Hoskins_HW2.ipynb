{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.metrics import accuracy_score # other metrics too pls!\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor # more!\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# adapt this code below to run your analysis\n",
    "# 1. Write a function to take a list or dictionary of clfs and hypers(i.e. use logistic regression), each with 3 different sets of hyper parameters for each\n",
    "# 2. Expand to include larger number of classifiers and hyperparameter settings\n",
    "# 3. Find some simple data\n",
    "# 4. generate matplotlib plots that will assist in identifying the optimal clf and parampters settings\n",
    "# 5. Please set up your code to be run and save the results to the directory that its executed from\n",
    "# 6. Investigate grid search function\n",
    "\n",
    "M = np.array([[1,2],[3,4],[4,5],[4,5],[4,5],[4,5],[4,5],[4,5]])\n",
    "L = np.ones(M.shape[0])\n",
    "n_folds = 5\n",
    "\n",
    "data = (M, L, n_folds)\n",
    "\n",
    "def run (clfs, data, clf_hyper={}):\n",
    "  M, L, n_folds = data # unpack data container\n",
    "  kf = KFold(n_splits=n_folds) # Establish the cross validation\n",
    "  ret = {} # classic explication of results\n",
    "\n",
    "  for ids, (train_index, test_index) in enumerate(kf.split(M, L)):\n",
    "    for clf in clfs:\n",
    "      clf = clf(**clf_hyper) # unpack parameters into clf if they exist\n",
    "      try:\n",
    "          clf.fit(M[train_index], L[train_index])\n",
    "      except ValueError:\n",
    "          pass\n",
    "      pred = clf.predict(M[test_index])\n",
    "      ret[ids]= {'clf': clf,\n",
    "                'train_index': train_index,\n",
    "                'test_index': test_index,\n",
    "                'accuracy': accuracy_score(L[test_index], pred)}\n",
    "    return ret\n",
    "\n",
    "clfs = ['LinearRegression','LogisticRegression','RandomForestClassifier']\n",
    "hypers = [  \n",
    "            {'C': [0.1,0.001,0.0001],\n",
    "             'penalty': ['l1','l2','elasticnet','none'],\n",
    "             'solver': ['lbfgs','liblinear','sag','saga']}\n",
    "#            {'n_estimators': 100, 'max_depth': 5}\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "results = run(clfs,data, clf_hyper={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import load_diabetes, load_iris,load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "#annoying so turned off for annoying convergence errors\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "\n",
    "logger = logging.getLogger('gridsearch')\n",
    "\n",
    "data = load_iris()\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#X = np.array([[1,2],[3,4],[4,5],[4,5],[4,5],[4,5],[4,5],[4,5]])\n",
    "#y = np.ones(X.shape[0])\n",
    "n_folds = 5\n",
    "\n",
    "models_params = {'LinearRegression': {'model': LinearRegression(), 'parameters': {'fit_intercept': [True, False], 'positive':[True,False],'n_jobs': [-1]}},\n",
    "              'RandomForestClassifier': {'model': RandomForestClassifier(), 'parameters': {'bootstrap':[True,False],'oob_score':[True,False],'criterion':['gini','entropy','log_loss'],'max_features':['sqrt','log2','None'],'n_estimators': [10, 50, 100], 'max_depth': [None, 5, 10],'n_jobs' :[-1]}},\n",
    "              'LogisticRegression': {'model': LogisticRegression(), 'parameters': {'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],'class_weight':['balanced','None'],'penalty': ['l1', 'l2', 'elasticnet', None], 'C': [0.01, 0.001, 0.1, 1, 10],'n_jobs':[-1]}},\n",
    "              'GradientBoostingRegressor': {'model':GradientBoostingRegressor(),'parameters':{'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],'learning_rate':[10,1,0.1,0.01,0.001],'criterion':['friedman_mse', 'squared_error', 'mse'],'max_features':['auto', 'sqrt', 'log2'],'alpha': [1.0,.8,.6,.4,.2,0.0]}}\n",
    "             }\n",
    "\n",
    "best_params = {}\n",
    "best_score = 0\n",
    "\n",
    "for model_name, model_dict in models_params.items():\n",
    "    model = model_dict['model']\n",
    "    params = model_dict['parameters']\n",
    "    for key, values in params.items():\n",
    "        for value in values:\n",
    "            parameters = {key: value}\n",
    "            score = train_and_evaluate(model_name, model, parameters, X,y)\n",
    "            if score is None:\n",
    "                logger.info(f'{model_name} has score of 0')\n",
    "                continue\n",
    "            if (model_name != 'LinearRegression' and score > best_score) or (model_name == 'LinearRegression' and score < best_score):\n",
    "                best_params = parameters\n",
    "                best_params['model_name'] = model_name\n",
    "                best_score = score\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\",best_score)\n",
    "\n",
    "\n",
    "def train_and_evaluate (model_name, model, parameters, X, y):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        try:\n",
    "            model.set_params(**parameters)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        except:\n",
    "            logger.info(f\"Error: One or more of the parameters {parameters} are not accepted by {model_name}\")\n",
    "            return None\n",
    "        if model_name == 'LinearRegression':\n",
    "            score = mean_squared_error(y_test, y_pred)\n",
    "        else:\n",
    "            score = model.score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import load_diabetes, load_iris,load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from itertools import product\n",
    "from time import sleep\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "ConvergenceWarning('ignore')\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "def _get_param_combinations(param_dict):\n",
    "    keys = param_dict.keys()\n",
    "    values = param_dict.values()\n",
    "    for item in product(*values):\n",
    "        yield dict(zip(keys, item))\n",
    "\n",
    "def _grid_search(X_data, y_data, models_params):\n",
    "    results = {}\n",
    "    for model_name, model_dict in models_params.items():\n",
    "        model = model_dict['model']\n",
    "        parameters = model_dict['parameters']\n",
    "        param_combinations = _get_param_combinations(parameters)\n",
    "        \n",
    "        for param_set in param_combinations:\n",
    "            model.set_params(**param_set)\n",
    "            \n",
    "            kf = KFold(n_splits=3)\n",
    "            for train_index, test_index in kf.split(X_data):\n",
    "                X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "                y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "                try:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    if model_name == 'LinearRegression':\n",
    "                        score = mean_squared_error(y_test, y_pred)\n",
    "                    else:\n",
    "                        score = model.score(X_test, y_test)\n",
    "                    if model_name not in results:\n",
    "                        results[model_name] = []\n",
    "                    results[model_name].append({'parameters': param_set, 'score': score})\n",
    "                    sleep(1)\n",
    "                except ValueError or UserWarning:\n",
    "                    pass\n",
    "    return results\n",
    "\n",
    "\n",
    "def full_grid_search(X_data, y_data, models_params):\n",
    "    results = _grid_search(X_data, y_data, models_params)\n",
    "    return results\n",
    "\n",
    "diab = load_diabetes()\n",
    "\n",
    "models_params = {   'LinearRegression':     {       \n",
    "                                                'model': LinearRegression(), \n",
    "                                                'parameters': {'fit_intercept': [True,False]}\n",
    "                                            },\n",
    "                    'RandomForestClassifier': { \n",
    "                                                'model': RandomForestClassifier(), \n",
    "                                                'parameters': { 'criterion':['gini','entropy','log_loss'],\n",
    "                                                                'max_features':['sqrt','log2'],\n",
    "                                                                'n_estimators': [10, 50, 100], \n",
    "                                                                'max_depth': [None, 5, 10]}\n",
    "                                                },\n",
    "                    'LogisticRegression': {\n",
    "                                                'model': LogisticRegression(), \n",
    "                                                'parameters': { 'solver':['lbfgs', 'sag', 'saga'],\n",
    "                                                                'class_weight':['balanced','None'],\n",
    "                                                                'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                                                                'C': [0.01, 0.001, 0.1, 1, 10],\n",
    "                                                                'max_iter': [50000]}\n",
    "                                            },\n",
    "                    'GradientBoostingRegressor': {  'model':GradientBoostingRegressor(),\n",
    "                                                    'parameters':{  'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "                                                                    'learning_rate':[10,1,0.1,0.01,0.001],\n",
    "                                                                    'criterion':['friedman_mse', 'squared_error'],\n",
    "                                                                    'max_features':['sqrt', 'log2'],\n",
    "                                                                    'alpha': [1.0,.8,.6,.4,.2,0.0]}\n",
    "                                                }\n",
    "                }\n",
    "\n",
    "\n",
    "results = full_grid_search(diab.data, diab.target, models_params)\n",
    "#print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parameters': {'fit_intercept': True}, 'score': 3034.2414203626154},\n",
       " {'parameters': {'fit_intercept': True}, 'score': 3253.20664674845},\n",
       " {'parameters': {'fit_intercept': True}, 'score': 2793.9946927742535},\n",
       " {'parameters': {'fit_intercept': False}, 'score': 29112.481780107624},\n",
       " {'parameters': {'fit_intercept': False}, 'score': 27192.41747970786},\n",
       " {'parameters': {'fit_intercept': False}, 'score': 26452.326079509967}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['LinearRegression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import load_diabetes, load_iris, load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pkl\n",
    "import logging\n",
    "import warnings\n",
    "from itertools import product\n",
    "from time import sleep\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "ConvergenceWarning('ignore')\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "def _get_param_combinations(param_dict):\n",
    "    keys = param_dict.keys()\n",
    "    values = param_dict.values()\n",
    "    for item in product(*values):\n",
    "        yield dict(zip(keys, item))\n",
    "\n",
    "def _grid_search(X_data, y_data, models_params,kf_splits=2):\n",
    "    results = {}\n",
    "    for model_name, model_dict in models_params.items():\n",
    "        model = model_dict['model']\n",
    "        parameters = model_dict['parameters']\n",
    "        param_combinations = _get_param_combinations(parameters)\n",
    "        \n",
    "        for param_set in param_combinations:\n",
    "            model.set_params(**param_set)\n",
    "            \n",
    "            kf = KFold(n_splits=kf_splits)\n",
    "            for train_index, test_index in kf.split(X_data):\n",
    "                X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "                y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "                try:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    if model_name in ['LinearRegression', 'GradientBoostingRegressor']:\n",
    "                        score = mean_absolute_error(y_test, y_pred)\n",
    "                    else:\n",
    "                        score = roc_auc_score(y_test, y_pred)\n",
    "                    if model_name not in results:\n",
    "                        results[model_name] = []\n",
    "                    results[model_name].append({'parameters': param_set, 'score': score})\n",
    "                    sleep(1)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        with open(os.path.join(os.getcwd(), 'results.pkl'), 'wb') as f:\n",
    "            pkl.dump(results, f)\n",
    "    return results\n",
    "\n",
    "\n",
    "def full_grid_search(X_data, y_data, models_params,kf_splits):\n",
    "    results = _grid_search(X_data, y_data, models_params,kf_splits=kf_splits)\n",
    "    return results\n",
    "\n",
    "def plot_scores_by_model(results, *model_names):\n",
    "    for model_name in model_names:\n",
    "        model_results = results[model_name]\n",
    "        scores = [result['score'] for result in model_results]\n",
    "        plt.plot(scores)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(model_name)\n",
    "        plt.savefig(f'{model_name}_grid_search_scores.png')\n",
    "    plt.show()\n",
    "\n",
    "diab = load_diabetes()\n",
    "\n",
    "models_params = {   'LinearRegression':     {       \n",
    "                                                'model': LinearRegression(), \n",
    "                                                'parameters': {'fit_intercept': [True,False]}\n",
    "                                            },\n",
    "                    'RandomForestClassifier': { \n",
    "                                                'model': RandomForestClassifier(), \n",
    "                                                'parameters': { 'criterion':['gini','entropy','log_loss'],\n",
    "                                                                'max_features':['sqrt','log2'],\n",
    "                                                                'n_estimators': [10, 50, 100], \n",
    "                                                                'max_depth': [None, 5, 10]}\n",
    "                                                },\n",
    "                    'LogisticRegression': {\n",
    "                                                'model': LogisticRegression(), \n",
    "                                                'parameters': { 'solver':['lbfgs', 'sag', 'saga'],\n",
    "                                                                'class_weight':['balanced','None'],\n",
    "                                                                'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                                                                'C': [0.01, 0.001, 0.1, 1, 10],\n",
    "                                                                'max_iter': [50000]}\n",
    "                                            },\n",
    "                    'GradientBoostingRegressor': {  'model':GradientBoostingRegressor(),\n",
    "                                                    'parameters':{  'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "                                                                    'learning_rate':[10,1,0.1,0.01,0.001],\n",
    "                                                                    'criterion':['friedman_mse', 'squared_error'],\n",
    "                                                                    'max_features':['sqrt', 'log2'],\n",
    "                                                                    'alpha': [1.0,.8,.6,.4,.2,0.0]}\n",
    "                                                }\n",
    "                }\n",
    "\n",
    "\n",
    "results = full_grid_search( diab.data,diab.target, models_params,kf_splits=3)\n",
    "\n",
    "#plot_scores_by_model(results, ('LogisticRegression','RandomForestClassifier','LinerRegression','GradientBoostinRegressor'))\n",
    "#plot_scores(results, 'RandomForestClassifier')\n",
    "#plot_scores(results, 'LogisticRegression')\n",
    "#plot_scores(results, 'GradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LinearRegression', 'GradientBoostingRegressor'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_scores_by_model(results, *model_names):\n",
    "    for model_name in model_names:\n",
    "        model_results = results[model_name]\n",
    "        scores = [result['score'] for result in model_results]\n",
    "        plt.plot(scores)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(model_name)\n",
    "        plt.savefig(f'{model_name}_grid_search_scores.png')\n",
    "    plt.show()\n",
    "plot_scores_by_model(results, 'GradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_splits=10 cannot be greater than the number of members in each class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=19'>20</a>\u001b[0m models_params \u001b[39m=\u001b[39m {   \u001b[39m'\u001b[39m\u001b[39mLinearRegression\u001b[39m\u001b[39m'\u001b[39m:     {       \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=20'>21</a>\u001b[0m                                                 \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: LinearRegression(), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=21'>22</a>\u001b[0m                                                 \u001b[39m'\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mfit_intercept\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39mTrue\u001b[39;00m,\u001b[39mFalse\u001b[39;00m]}\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=44'>45</a>\u001b[0m                                                 }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=45'>46</a>\u001b[0m                 }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=48'>49</a>\u001b[0m diab \u001b[39m=\u001b[39m load_diabetes()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=50'>51</a>\u001b[0m results \u001b[39m=\u001b[39m evaluate_models(diab\u001b[39m.\u001b[39;49mdata,diab\u001b[39m.\u001b[39;49mtarget,models_params)\n",
      "\u001b[1;32m/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb Cell 8\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(X, y, param_grid)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=11'>12</a>\u001b[0m         params \u001b[39m=\u001b[39m model_params[\u001b[39m'\u001b[39m\u001b[39mparameters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=12'>13</a>\u001b[0m         grid_search \u001b[39m=\u001b[39m GridSearchCV(model, params, cv\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=13'>14</a>\u001b[0m         grid_search\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=14'>15</a>\u001b[0m         results[model_name] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mbest_score\u001b[39m\u001b[39m'\u001b[39m: grid_search\u001b[39m.\u001b[39mbest_score_, \u001b[39m'\u001b[39m\u001b[39mbest_params\u001b[39m\u001b[39m'\u001b[39m: grid_search\u001b[39m.\u001b[39mbest_params_}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=15'>16</a>\u001b[0m \u001b[39m#        print(f'{model_name} best score: {grid_search.best_score_}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/allen/Desktop/MSDS/ML2/ml2_smu/Homeworks/Allen_Hoskins_HW2.ipynb#ch0000012?line=16'>17</a>\u001b[0m \u001b[39m#        print(f'{model_name} best parameters: {grid_search.best_params_}')\u001b[39;00m\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_search.py:834\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_split.py:340\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n\u001b[1;32m    333\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         (\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have number of splits n_splits=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m greater\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m than the number of samples: n_samples=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         )\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    338\u001b[0m     )\n\u001b[0;32m--> 340\u001b[0m \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    341\u001b[0m     \u001b[39myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_split.py:86\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     84\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m     85\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(_num_samples(X))\n\u001b[0;32m---> 86\u001b[0m \u001b[39mfor\u001b[39;00m test_index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[1;32m     87\u001b[0m     train_index \u001b[39m=\u001b[39m indices[np\u001b[39m.\u001b[39mlogical_not(test_index)]\n\u001b[1;32m     88\u001b[0m     test_index \u001b[39m=\u001b[39m indices[test_index]\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_split.py:713\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iter_test_masks\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 713\u001b[0m     test_folds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_test_folds(X, y)\n\u001b[1;32m    714\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits):\n\u001b[1;32m    715\u001b[0m         \u001b[39myield\u001b[39;00m test_folds \u001b[39m==\u001b[39m i\n",
      "File \u001b[0;32m~/virtualenvs/ML1/lib/python3.9/site-packages/sklearn/model_selection/_split.py:675\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    673\u001b[0m min_groups \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmin(y_counts)\n\u001b[1;32m    674\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m y_counts):\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    676\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_splits=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m cannot be greater than the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    677\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m number of members in each class.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits)\n\u001b[1;32m    678\u001b[0m     )\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m min_groups:\n\u001b[1;32m    680\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe least populated class in y has only \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m members, which is less than n_splits=\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m         \u001b[39m%\u001b[39m (min_groups, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits),\n\u001b[1;32m    684\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m    685\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: n_splits=10 cannot be greater than the number of members in each class."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_diabetes, load_iris, load_digits\n",
    "\n",
    "def evaluate_models(X, y, param_grid,cv=2):\n",
    "    results = {}\n",
    "    # Iterate over the models and their corresponding parameter grids\n",
    "    for model_name, model_params in param_grid.items():\n",
    "        model = model_params['model']\n",
    "        params = model_params['parameters']\n",
    "        grid_search = GridSearchCV(model, params, cv=cv)\n",
    "        grid_search.fit(X, y)\n",
    "        results[model_name] = {'best_score': grid_search.best_score_, 'best_params': grid_search.best_params_}\n",
    "#        print(f'{model_name} best score: {grid_search.best_score_}')\n",
    "#        print(f'{model_name} best parameters: {grid_search.best_params_}')\n",
    "    return results\n",
    "\n",
    "models_params = {   'LinearRegression':     {       \n",
    "                                                'model': LinearRegression(), \n",
    "                                                'parameters': {'fit_intercept': [True,False]}\n",
    "                                            },\n",
    "                    'RandomForestClassifier': { \n",
    "                                                'model': RandomForestClassifier(), \n",
    "                                                'parameters': { 'criterion':['gini','entropy','log_loss'],\n",
    "                                                                'max_features':['sqrt','log2'],\n",
    "                                                                'n_estimators': [10, 50, 100], \n",
    "                                                                'max_depth': [None, 5, 10]}\n",
    "                                                },\n",
    "                    'LogisticRegression': {\n",
    "                                                'model': LogisticRegression(), \n",
    "                                                'parameters': { 'solver':['lbfgs', 'sag', 'saga'],\n",
    "                                                                'class_weight':['balanced','None'],\n",
    "                                                                'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                                                                'C': [0.01, 0.001, 0.1, 1, 10],\n",
    "                                                                'max_iter': [50000]}\n",
    "                                            },\n",
    "                    'GradientBoostingRegressor': {  'model':GradientBoostingRegressor(),\n",
    "                                                    'parameters':{  'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "                                                                    'learning_rate':[10,1,0.1,0.01,0.001],\n",
    "                                                                    'criterion':['friedman_mse', 'squared_error'],\n",
    "                                                                    'max_features':['sqrt', 'log2'],\n",
    "                                                                    'alpha': [1.0,.8,.6,.4,.2,0.0]}\n",
    "                                                }\n",
    "                }\n",
    "\n",
    "\n",
    "diab = load_diabetes()\n",
    "\n",
    "results = evaluate_models(diab.data,diab.target,models_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, f1_score,log_loss\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import load_diabetes, load_iris, load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle as pkl\n",
    "import logging\n",
    "import warnings\n",
    "from itertools import product\n",
    "from time import sleep\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "ConvergenceWarning('ignore')\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "def run(X_data, y_data, models_params,kf_splits):\n",
    "\n",
    "    diab = load_diabetes()\n",
    "\n",
    "    models_params = {   'LinearRegression':     {       \n",
    "                                                'model': LinearRegression(), \n",
    "                                                'parameters': {'fit_intercept': [True,False]}\n",
    "                                            },\n",
    "                    'RandomForestClassifier': { \n",
    "                                                'model': RandomForestClassifier(), \n",
    "                                                'parameters': { 'criterion':['gini','entropy','log_loss'],\n",
    "                                                                'max_features':['sqrt','log2'],\n",
    "                                                                'n_estimators': [10, 50, 100], \n",
    "                                                                'max_depth': [None, 5, 10]}\n",
    "                                                },\n",
    "                    'LogisticRegression': {\n",
    "                                                'model': LogisticRegression(), \n",
    "                                                'parameters': { 'solver':['lbfgs', 'sag', 'saga'],\n",
    "                                                                'class_weight':['balanced','None'],\n",
    "                                                                'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                                                                'C': [0.01, 0.001, 0.1, 1, 10],\n",
    "                                                                'max_iter': [50000]}\n",
    "                                            },\n",
    "                    'GradientBoostingRegressor': {  'model':GradientBoostingRegressor(),\n",
    "                                                    'parameters':{  'loss':['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "                                                                    'learning_rate':[10,1,0.1,0.01,0.001],\n",
    "                                                                    'criterion':['friedman_mse', 'squared_error'],\n",
    "                                                                    'max_features':['sqrt', 'log2'],\n",
    "                                                                    'alpha': [1.0,.8,.6,.4,.2,0.0]}\n",
    "                                                }\n",
    "                }\n",
    "\n",
    "\n",
    "    results = full_grid_search( diab.data,diab.target, models_params,kf_splits=3)\n",
    "    results.keys()\n",
    "\n",
    "def _get_param_combinations(param_dict):\n",
    "    keys = param_dict.keys()\n",
    "    values = param_dict.values()\n",
    "    for item in product(*values):\n",
    "        yield dict(zip(keys, item))\n",
    "\n",
    "def grid_search(X_data, y_data, models_params,kf_splits=2):\n",
    "    results = {}\n",
    "    for model_name, model_dict in models_params.items():\n",
    "        model = model_dict['model']\n",
    "        parameters = model_dict['parameters']\n",
    "        param_combinations = _get_param_combinations(parameters)\n",
    "        \n",
    "        for param_set in param_combinations:\n",
    "            model.set_params(**param_set)\n",
    "            \n",
    "            kf = KFold(n_splits=kf_splits)\n",
    "            for train_index, test_index in kf.split(X_data):\n",
    "                X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "                y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "                try:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    if model_name in ['LinearRegression', 'GradientBoostingRegressor']:\n",
    "                        score = mean_absolute_error(y_test, y_pred)\n",
    "                    elif model_name in ['RandomForestClassifier','LogisticRegression']:\n",
    "                        if model_name == 'LogisticRegression':\n",
    "                            score = {\n",
    "                                    'f1_score': f1_score(y_test, y_pred > 0.5),\n",
    "                                    'log_loss': log_loss(y_test, y_pred)}\n",
    "                        else:\n",
    "                            score = roc_auc_score(y_test, y_pred)\n",
    "                    if model_name not in results:\n",
    "                        results[model_name] = []\n",
    "                    results[model_name].append({'parameters': param_set, 'score': score})\n",
    "                    #had IO issue so adding in 1 second sleep to get around this\n",
    "                    sleep(1)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        with open(os.path.join(os.getcwd(), 'results.pkl'), 'wb') as f:\n",
    "            pkl.dump(results, f)\n",
    "    return results\n",
    "\n",
    "\n",
    "def full_grid_search(X_data, y_data, models_params,kf_splits):\n",
    "    results = _grid_search(X_data, y_data, models_params,kf_splits=kf_splits)\n",
    "    return results\n",
    "\n",
    "def plot_scores_by_model(results, *model_names):\n",
    "    for model_name in model_names:\n",
    "        model_results = results[model_name]\n",
    "        scores = [result['score'] for result in model_results]\n",
    "        plt.plot(scores)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(model_name)\n",
    "        plt.savefig(f'{model_name}_grid_search_scores.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_data, y_data, models_params,kf_splits=2):\n",
    "    results = {}\n",
    "    for model_name, model_dict in models_params.items():\n",
    "        model = model_dict['model']\n",
    "        parameters = model_dict['parameters']\n",
    "        param_combinations = _get_param_combinations(parameters)\n",
    "        \n",
    "        for param_set in param_combinations:\n",
    "            model.set_params(**param_set)\n",
    "            \n",
    "            kf = KFold(n_splits=kf_splits)\n",
    "            for train_index, test_index in kf.split(X_data):\n",
    "                X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "                y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "                try:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    if model_name in ['LinearRegression', 'GradientBoostingRegressor']:\n",
    "                        score = mean_absolute_error(y_test, y_pred)\n",
    "                    elif model_name in ['RandomForestClassifier','LogisticRegression']:\n",
    "                        if model_name == 'LogisticRegression':\n",
    "                            score = {'roc_auc': roc_auc_score(y_test, y_pred),\n",
    "                                    'f1_score': f1_score(y_test, y_pred > 0.5),\n",
    "                                    'log_loss': log_loss(y_test, y_pred)}\n",
    "                        else:\n",
    "                            score = roc_auc_score(y_test, y_pred)\n",
    "                    if model_name not in results:\n",
    "                        results[model_name] = []\n",
    "                    results[model_name].append({'parameters': param_set, 'score': score})\n",
    "                    #had IO issue so adding in 1 second sleep to get around this\n",
    "                    sleep(1)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        with open(os.path.join(os.getcwd(), 'results.pkl'), 'wb') as f:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('LinearRegression', {'model': LinearRegression(), 'parameters': {'fit_intercept': [True, False]}}),\n",
    " ('LogisticRegression', {'model': LogisticRegression(), \n",
    "                        'parameters': { 'solver': ['sag', 'saga'],\n",
    "                                         'class_weight': ['balanced', 'None'],\n",
    "                                          'penalty': ['l1', 'l2', 'elasticnet', None], \n",
    "                                          'C': [0.01, 0.001, 0.1, 1, 10],\n",
    "                                           'max_iter': [50000]}}), \n",
    "('GradientBoostingRegressor', {'model': GradientBoostingRegressor(),\n",
    "                                 'parameters': {'loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "                                                 'learning_rate': [10, 1, 0.1, 0.01, 0.001],\n",
    "                                                  'criterion': ['friedman_mse', 'squared_error'], \n",
    "                                                  'max_features': ['sqrt', 'log2'], \n",
    "                                                  'alpha': [1.0, 0.8, 0.6, 0.4, 0.2, 0.0]}})]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3121556d4473c412f0595648b5a85484255f0878c67145bad0a6023335311018"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
